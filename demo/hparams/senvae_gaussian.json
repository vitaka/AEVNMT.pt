{
    "attention": "bahdanau",
    "batch_size": 80,
    "beam_width": 10,
    "bidirectional": true,
    "cell_type": "lstm",
    "decoder_style": "bahdanau",
    "dropout": 0.5,
    "emb_init_scale": 0.01,
    "emb_size": 256,
    "evaluate_every": -1,
    "hidden_size": 256,
    "latent_size": 32,
    "criterion": "likelihood",
    "gen_optimizer": "adam",
    "gen_lr": 0.002,
    "gen_l2_weight": 0.0,
    "inf_z_optimizer": "adam",
    "inf_z_lr": 0.002,
    "inf_z_l2_weight": 0.0,
    "length_penalty_factor": 1.0,
    "lr_reduce_cooldown": 2,
    "lr_reduce_factor": 0.5,
    "lr_reduce_patience": 5,
    "max_gradient_norm": 4.0,
    "max_sentence_length": 50,
    "max_decoding_length": 50,
    "max_vocab_size": -1,
    "min_lr": 6.25e-05,
    "model_type": "aevnmt",
    "num_dec_layers": 1,
    "num_enc_layers": 1,
    "num_epochs": 15,
    "patience": 9999999999,
    "cooldown_patience": 5,
    "print_every": 100,
    "share_vocab": false,
    "src": "en",
    "tgt": "en",
    "training_prefix": "",
    "subword_token": "@@",
    "tied_embeddings": true,
    "use_gpu": false,
    "vocab_min_freq": 0,
    "word_dropout": 0.0,
    "KL_annealing_steps": 0,
    "KL_free_nats": 0.0, 
    "prior": "gaussian",
    "prior_params": [0.0, 1.0], 
    "posterior": "gaussian", 
    "inf_encoder_style": "rnn",  
    "inf_conditioning": "x",
    "feed_z": false,
    "max_pooling_states": false
}
